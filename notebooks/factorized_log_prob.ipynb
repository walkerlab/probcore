{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook demonstrates the use of the factorized log prob method in gensn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributions as D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensn.distributions import TrainableDistributionAdapter, Joint\n",
    "from gensn.parameters import TransformedParameter\n",
    "from gensn.variational import ELBOMarginal, VariationalDequantizedDistribution\n",
    "from gensn.flow import FlowDistribution\n",
    "import gensn.transforms.invertible as T\n",
    "import gensn.distributions as G\n",
    "import torch.distributions as D\n",
    "\n",
    "from gensn.utils import squeeze_tuple, turn_to_tuple\n",
    "\n",
    "\n",
    "seed = 100\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorized log prob for independent distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a 2d independent distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 2\n",
    "loc = torch.zeros(n_dims)\n",
    "scale = torch.ones(n_dims)\n",
    "dist = G.IndependentNormal(loc, scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call log prob and factorized log prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.8379) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(n_dims)\n",
    "lp = dist.log_prob(x)\n",
    "print(lp, lp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9189, -0.9189]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "flp = dist.factorized_log_prob(x)\n",
    "print(flp, flp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isclose(flp.sum(), lp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorized log prob for flow distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First create factorized transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = T.IndependentAffine(n_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare an initializer\n",
    "init_std = 0.1\n",
    "def init_module(module):\n",
    "    if isinstance(module, T.IndependentAffine):\n",
    "        module.weight.data.normal_(mean=1.0, std=init_std)\n",
    "        module.bias.data.normal_(std=init_std * 0.1)\n",
    "\n",
    "affine.apply(init_module);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call forward and factorized forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0039,  0.0024], grad_fn=<AddBackward0>) torch.Size([2])\n",
      "tensor(0.0064, grad_fn=<SumBackward1>) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "y, log_det = affine(x)\n",
    "print(y, y.shape)\n",
    "print(log_det, log_det.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0039,  0.0024], grad_fn=<AddBackward0>) torch.Size([2])\n",
      "tensor([ 0.0354, -0.0290], grad_fn=<MulBackward0>) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "fy, flog_det = affine.factorized_forward(x)\n",
    "print(fy, fy.shape)\n",
    "print(flog_det, flog_det.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isclose(flog_det.sum(), log_det)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_affine = T.InverseTransform(T.IndependentAffine(n_dims))\n",
    "\n",
    "inv_affine.apply(init_module);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0037, 0.0113], grad_fn=<DivBackward0>) torch.Size([2])\n",
      "tensor(0.4120, grad_fn=<SumBackward1>) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "y, log_det = inv_affine(x)\n",
    "print(y, y.shape)\n",
    "print(log_det, log_det.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0037, 0.0113], grad_fn=<DivBackward0>) torch.Size([2])\n",
      "tensor([0.1489, 0.2631], grad_fn=<NegBackward0>) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "fy, flog_det = inv_affine.factorized_forward(x)\n",
    "print(fy, fy.shape)\n",
    "print(flog_det, flog_det.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isclose(flog_det.sum(), log_det)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sequential transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_sequence = [\n",
    "    T.InverseTransform(T.Softplus()),\n",
    "    T.IndependentAffine(n_dims),\n",
    "    T.ELU(),\n",
    "    T.IndependentAffine(n_dims),\n",
    "    T.ELU(),\n",
    "    T.IndependentAffine(n_dims),\n",
    "]\n",
    "\n",
    "sequential = T.SequentialTransform(*transform_sequence)\n",
    "\n",
    "sequential.apply(init_module);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5234, -0.6372], grad_fn=<AddBackward0>) torch.Size([2])\n",
      "tensor(-14.7422, grad_fn=<AddBackward0>) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "y, log_det = sequential(x)\n",
    "print(y, y.shape)\n",
    "print(log_det, log_det.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5234, -0.6372], grad_fn=<AddBackward0>) torch.Size([2])\n",
      "tensor([-16.1689,   1.4267], grad_fn=<AddBackward0>) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "fy, flog_det = sequential.factorized_forward(x)\n",
    "print(fy, fy.shape)\n",
    "print(flog_det, flog_det.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isclose(flog_det.sum(), log_det)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a flow distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_base_dist = G.IndependentNormal(loc, scale)\n",
    "flow_dist = FlowDistribution(flow_base_dist, sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-16.9201, grad_fn=<AddBackward0>) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "lp = flow_dist.log_prob(x)\n",
    "print(lp, lp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-17.2248,   0.3048], grad_fn=<AddBackward0>) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "flp = flow_dist.factorized_log_prob(x)\n",
    "print(flp, flp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isclose(flp.sum(), lp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorized elbo for variational dequantized distributions with normalizing flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_base_dist = G.IndependentNormal(\n",
    "    loc=torch.zeros(n_dims), scale=torch.ones(n_dims)\n",
    ")\n",
    "prior_dist = FlowDistribution(prior_base_dist, sequential)\n",
    "\n",
    "dequant_base_dist = G.IndependentLaplace(\n",
    "    loc=torch.zeros(n_dims),\n",
    "    scale=torch.ones(n_dims),\n",
    ")\n",
    "dequant_dist = FlowDistribution(dequant_base_dist, sequential)\n",
    "\n",
    "vdd = VariationalDequantizedDistribution(\n",
    "    prior=prior_dist,\n",
    "    dequantizer=dequant_dist,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1279, grad_fn=<MeanBackward1>) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "elbo = vdd(x, n_samples=10_000_000)\n",
    "print(elbo, elbo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-17.2248,   0.3048], grad_fn=<AddBackward0>) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "felbo = vdd.factorized_elbo(x, n_samples=10_000_000)\n",
    "print(flp, flp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0010, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "felbo.sum() - elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39misclose(felbo\u001b[39m.\u001b[39msum(), elbo)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert torch.isclose(felbo.sum(), elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1083, grad_fn=<SubBackward0>) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "iw_bound = vdd.iw_bound(x, n_samples=10_000_000)\n",
    "print(iw_bound, iw_bound.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0466, 0.0617], grad_fn=<SubBackward0>) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "fiw_bound = vdd.factorized_iw_bound(x, n_samples=10_000_000)\n",
    "print(fiw_bound, fiw_bound.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.6703e-05, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiw_bound.sum() - iw_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39misclose(fiw_bound\u001b[39m.\u001b[39msum(), iw_bound)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert torch.isclose(fiw_bound.sum(), iw_bound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
